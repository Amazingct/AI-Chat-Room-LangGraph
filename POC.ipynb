{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, validator\n",
    "load_dotenv()\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "import os, time\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "from langchain_core.agents import AgentFinish\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.graph import END, StateGraph\n",
    "import json\n",
    "\n",
    "import functools\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "import operator\n",
    "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define graph state\n",
    "class AgentState(TypedDict):\n",
    "    chat_history: list[BaseMessage]\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # sender: str\n",
    "    user_config:dict\n",
    "\n",
    "\n",
    "class OutputObject(BaseModel):\n",
    "    response: str = Field(description=\"your contribution or response; this should be empty when you are raising your hand(s)\")\n",
    "    hand: bool = Field(description=\"set to true if your hand is raised else false\", default=False)\n",
    "    sender: str = Field(description=\"your name\")\n",
    "    directed_to: str = Field(description=\"if your response is directed to anyone specific in the room, your specify that here using their name else set this to general\", default=\"general\")\n",
    "    go_ahead: bool = Field(description=\"set to true if you want the expert to speak (This is for the moderator only)\", default=False)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.dict())\n",
    "\n",
    "# Set up a parser \n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=OutputObject)\n",
    "format_instructions = pydantic_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class OutputObject2(BaseModel):\n",
    "    response: str = Field(description=\"your response/message\")\n",
    "    sender: str = Field(description=\"your name\")\n",
    "    directed_to: str = Field(description=\"if your response is directed to anyone specific in the room, your specify that here using their name else set this to general\", default=\"general\")\n",
    "    go_ahead: bool = Field(description=\"set to true if you want the expert to speak\", default=False)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.dict())\n",
    "\n",
    "# Set up a parser \n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=OutputObject2)\n",
    "moderator_format_instructions = pydantic_parser.get_format_instructions()\n",
    "\n",
    "other_experts = [{\n",
    "    \"name\":\"mr remi\",\n",
    "    \"field\": \"solar and inverter\"},\n",
    "                 \n",
    "    {\n",
    "    \"name\":\"mr bayo\",\n",
    "    \"field\": \"smart lights\"},\n",
    "    \n",
    "    {\"name\":\"moderator\",\n",
    "    \"field\": \"moderating the conversation\"},\n",
    "                 \n",
    "                 ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_agent(llm, avatar,expert_in, tools=[], experts=other_experts, response_format=format_instructions, system_message: str=None):\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "    functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You name is {avatar} (an expert in  {expert_in}), in a room with other experts on diffrent fields.\" \n",
    "                \"You are gathered in this room to just share knowlege and talk about any topic that might arise.\"\n",
    "                \"There is a moderator that oversees the conversaton, so when you feel you have something to contribute,\"\n",
    "                \"raise up your hands only (do not speak yet),\"\n",
    "                \"then you wait for the moderator to give you a 'go_ahead' to speak before you do.\"\n",
    "                \"Specify if the response is to everyone or directed to a specific expert,\"\n",
    "                \"Here is the format for every single message you send: {response_format}.\"\n",
    "                \"Here is the complete list of experts in the room {experts}\"\n",
    "                \n",
    "                \n",
    "                #\" If you or any of the other assistants have the final answer or deliverable i.e the test cases,\"\n",
    "                #\" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                #\" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(avatar=avatar)\n",
    "    prompt = prompt.partial(expert_in=expert_in)\n",
    "    prompt = prompt.partial(response_format=response_format)\n",
    "    prompt = prompt.partial(experts=experts)\n",
    "    #prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    return prompt | llm #.bind_functions(functions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to create a node for a given agent\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    # We convert the agent output into a format that is suitable to append to the global state\n",
    "    if isinstance(result, FunctionMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        # Since we have a strict workflow, we can\n",
    "        # track the sender so we know who to pass to next.\n",
    "        #\"sender\": name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLAR SYSTEM AND INVERTER EXPERT\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", model_kwargs = {\"response_format\":{\"type\": \"json_object\"}})\n",
    "#system_message_=\"\"\n",
    "solar_expert = create_agent(\n",
    "    llm=llm,\n",
    "    avatar=\"mr remi\",\n",
    "    expert_in = \"solar/inverter solutions for homes\"\n",
    ")\n",
    "solar_expert_node = functools.partial(agent_node, agent=solar_expert, name=\"mr remi\")\n",
    "\n",
    "\n",
    "\n",
    "#SMART LIGHTS EXPERT\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", model_kwargs = {\"response_format\":{\"type\": \"json_object\"}})\n",
    "#system_message_=\"\"\n",
    "lights_expert = create_agent(\n",
    "    llm=llm,\n",
    "    avatar=\"mr bayo\",\n",
    "    expert_in = \"smart lights expert\"\n",
    ")\n",
    "lights_expert_node = functools.partial(agent_node, agent=lights_expert, name=\"mr bayo\")\n",
    "\n",
    "\n",
    "\n",
    "#MODERATOR\n",
    "llm_moderator = ChatOpenAI(model=\"gpt-4-1106-preview\", model_kwargs = {\"response_format\":{\"type\": \"json_object\"}})\n",
    "\"\"\"Create an agent.\"\"\"\n",
    "def mod(llm=llm_moderator, avatar=\"moderator\",expert_in=\"organising conversations\", tools=[], experts=other_experts, response_format=moderator_format_instructions, system_message: str=None):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You name is {avatar} (an expert in  {expert_in}), in a room with other experts on diffrent fields. \" \n",
    "                \"You are gathered in this room to just share knowlege and talk about any topic that might arise. \"\n",
    "                \"You are the moderator that oversees the conversaton. \"\n",
    "                \"You are to decide what to do next, or who to speak next. \"\n",
    "                \"Experts will raise up their hands only, when they want to say something, \"\n",
    "                \"it is your job to give a go_ahead if you want them to speak next. \"\n",
    "                \"you must Specify who yourresponse is; any of the experts. \"\n",
    "                \n",
    "                \"Here is the format for every single message you send: {response_format}. \"\n",
    "                \"Here is the complete list of experts in the room {experts}. \"\n",
    "                \"For now, just let them take turns sequentially as the appear, and start all over again.\"\n",
    "                \n",
    "                \n",
    "                #\" If you or any of the other assistants have the final answer or deliverable i.e the test cases,\"\n",
    "                #\" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                #\" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(avatar=avatar)\n",
    "    prompt = prompt.partial(expert_in=expert_in)\n",
    "    prompt = prompt.partial(response_format=response_format)\n",
    "    prompt = prompt.partial(experts=experts)\n",
    "    #prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    return prompt | llm #.bind_functions(functions)\n",
    "\n",
    "\n",
    "moderator = mod()\n",
    "moderator_node = functools.partial(agent_node, agent=moderator, name=\"moderator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "experts_names = [expert[\"name\"] for expert in other_experts]\n",
    "experts_names.remove(\"moderator\")\n",
    "\n",
    "#Edges logic\n",
    "def moderator_to_expert_edge_logic(state):\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = json.loads(messages[-1].content)\n",
    "    \n",
    "    \n",
    "    if last_message[\"sender\"] !=\"moderator\": #any of the experts\n",
    "        return \"moderator\" #send to moderator for broadcast\n",
    "    elif last_message['directed_to'] ==\"general\" : #moderator directs message to general then ramdomly pick an expert\n",
    "         return random.choice(experts_names)\n",
    "    else:\n",
    "        return last_message['directed_to'] #send to who it is meant for\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"mr bayo\",lights_expert_node)\n",
    "workflow.add_node(\"moderator\",moderator_node)\n",
    "workflow.add_node(\"mr remi\",solar_expert_node)\n",
    "workflow.set_entry_point(\"moderator\")\n",
    "\n",
    "\n",
    "#EDGES\n",
    "workflow.add_conditional_edges(\n",
    "    \"moderator\",\n",
    "    moderator_to_expert_edge_logic,\n",
    "    {expert: expert for expert in experts_names} #go to who the message was directed to\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i, expert in enumerate(experts_names):\n",
    "    workflow.add_edge(\n",
    "    expert,\n",
    "    \"moderator\"\n",
    ")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: moderator\n",
      "Response: Thank you for the topic. Let's explore the value of having a smart home. Mr. Remi, with your expertise in solar and inverter technology, can you share your insights on how smart homes might integrate renewable energy solutions? Your perspective would be valuable to this discussion.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "start_message = {\n",
    "  \"topic\": \"The topic is 'Does having a smart home worth it?' \",\n",
    "}\n",
    "\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=f'''{start_message[\"topic\"]}'''\n",
    "            )\n",
    "        ],\n",
    "        \n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    name = list(s.keys())[0]\n",
    "    content = json.loads(s[name][\"messages\"][0].content)[\"response\"]\n",
    "    try:\n",
    "        hand = json.loads(s[name][\"messages\"][0].content)[\"hand\"]\n",
    "    except:\n",
    "        hand = False\n",
    "    print(\"Name:\", name)\n",
    "    if hand:\n",
    "        print(\"Notification:\", f''' {name} is raising his/her hands''')\n",
    "    if content !=\"\":\n",
    "          print(\"Response:\", content)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = {expert: expert for expert in experts_names}\n",
    "ee.update({\"end\":END})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mr remi': 'mr remi', 'mr bayo': 'mr bayo', 'end': '__end__'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
